import streamlit as st
import pickle
import pandas as pd
import numpy as np 
import nltk as nlt 
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer

st.title('–§–µ–π–∫–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏')

title = st.text_input('–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É')
title = [title]
#–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache(allow_output_mutation=True)
def load_model():
    model = pickle.load(open('phish.pkl', 'rb'))
    return model

model = load_model()
result = st.sidebar.button('ü§ó–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å')
label = model.predict(title)
st.write(label)  

tokenizer = RegexpTokenizer(r'[A-Za-z]+') #[a-zA-Z]–æ–±–æ–∑–Ω–∞—á–∞–µ—Ç –æ–¥–∏–Ω —Å–∏–º–≤–æ–ª –æ—Ç a –¥–æ z –∏–ª–∏ –æ—Ç A –¥–æZ
stemmer = SnowballStemmer("english")
cv = CountVectorizer()

#–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –°–ü–ê–ú–∞

def prepare_data(title):
    if not title:
        return None, None
    X = pd.DataFrame({'url': [title]})
    X['text_tokenized'] = X.url.map(lambda t: tokenizer.tokenize(t)) #–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç–æ–∫–µ–Ω—ã
    X['text_stemmed'] = X.text_tokenized.map(lambda t: [stemmer.stem(word) for word in t])#stemmer –ø—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ —Å –æ–¥–Ω–∏–º –∫–æ—Ä–Ω–µ–º –∫ –æ–¥–Ω–æ–º—É —Å–ª–æ–≤—É
    X['text_sent'] = X.text_stemmed.map(lambda t: ' '.join(t)) #–û–±—ä–µ–¥–µ–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    features = cv.fit_transform(X.text_sent)
    return X, features

X, features = prepare_data(title)

if result and features is not None:
    model = load_model()
    y_pred = model.predict(features)
    if y_pred[0] == 0:
        st.write('–≠—Ç–æ –Ω–µ —Å–ø–∞–º!')
    else:
        st.write('–≠—Ç–æ —Å–ø–∞–º!')
