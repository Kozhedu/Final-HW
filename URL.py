import streamlit as st
import pickle
import pandas as pd
import numpy as np 
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer
st.title('–§–µ–π–∫–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏')
title = st.text_input('–í–≤–µ–¥–∏—Ç–µ —Å—Å—ã–ª–∫—É', value='https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html')

#–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
@st.cache(allow_output_mutation=True)

def load_model():
    movies = pickle.load(open('myfile.pkl','rb'))
    model = pickle.load(movies)
    return model

result = st.sidebar.button('ü§ó–†–∞—Å–ø–æ–∑–Ω–∞—Ç—å')

# —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–æ–π 'url'
data = {'url': [title]}
df = pd.DataFrame(data)
X = df[['url']].copy()

tokenizer = RegexpTokenizer(r'[A-Za-z]+') #[a-zA-Z]–æ–±–æ–∑–Ω–∞—á–∞–µ—Ç –æ–¥–∏–Ω —Å–∏–º–≤–æ–ª –æ—Ç a –¥–æ z –∏–ª–∏ –æ—Ç A –¥–æZ
stemmer = SnowballStemmer("english")
cv = CountVectorizer()

#–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –°–ü–ê–ú–∞
def prepare_data(X) :
    X['text_tokenized'] = X.url.map(lambda t: tokenizer.tokenize(t)) #–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç–æ–∫–µ–Ω—ã
    X['text_stemmed'] = X.text_tokenized.map(lambda t: [stemmer.stem(word) for word in t])#stemmer –ø—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ —Å –æ–¥–Ω–∏–º –∫–æ—Ä–Ω–µ–º –∫ –æ–¥–Ω–æ–º—É —Å–ª–æ–≤—É
    X['text_sent'] = X.text_stemmed.map(lambda t: ' '.join(t)) #–û–±—ä–µ–¥–µ–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    features = cv.fit_transform(X.text_sent)
    return X, features
X, features = prepare_data(X)

if result:
    model = load_model()
    y_pred = model.predict(features)
    if y_pred[0] == 0:
        st.write('–≠—Ç–æ –Ω–µ —Å–ø–∞–º!')
    else:
        st.write('–≠—Ç–æ —Å–ø–∞–º!')